# -*- coding: utf-8 -*-
"""stockmarket

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/150bKFeGWaj1jG0NVPcm8F-1pn3jj-dSG
"""

import numpy as np
import json
#import pandas as pd
import yfinance as yf
import tensorflow as tf
import matplotlib.pyplot as plt
import os

from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.layers import LSTM, Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from sklearn.preprocessing import MinMaxScaler

# Enable eager execution (fix TensorFlow execution issue)
tf.compat.v1.enable_eager_execution()
#"GOOG", "AAPL", "MSFT", "META", "TSLA", "TWTR"
companies = ["GOOG", "AAPL", "MSFT", "META", "TSLA",  "NVDA", "AMZN", "ORCL", "COST", "NFLX", "CSCO", "TM", "IBM", "AMD", "INTC", "WMT", "SBUX", "MCD", "NTDOY"]

for ticker in companies:
  df = yf.Ticker(ticker).history(period="max")

  data = df[['Close']].values

  # Normalize data
  scaler = MinMaxScaler(feature_range=(0, 1))
  scaled_data = scaler.fit_transform(data)

  # Define sequence length (increased to capture longer trends)
  sequence_length = 120

  # Create training data
  X_train, y_train = [], []
  for i in range(sequence_length, len(scaled_data) - 30):
      X_train.append(scaled_data[i-sequence_length:i, 0])
      y_train.append(scaled_data[i, 0])

  X_train, y_train = np.array(X_train), np.array(y_train)
  X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))  # Reshape for LSTM

  # Model filename
  model_filename = "stock_model_close_optimized.h5"

  #  Check if model exists
  if os.path.exists(model_filename):
      print(" Loading existing model for further training...")
      try:
          model = load_model(model_filename)
          model.compile(optimizer='adam', loss='huber_loss', metrics=['mae'])  # ðŸ”¹ Use Huber loss for stability
          extra_epochs = 15  # Continue training
      except Exception as e:
          print(f"Model loading failed: {e}. Re-training from scratch.")
          os.remove(model_filename)  # Delete corrupted model
          model = None
          extra_epochs = 50
  else:
      model = None
      extra_epochs = 50

  # Define new model if needed
  if model is None:
      print(" Training new model...")
      model = Sequential([
          LSTM(200, return_sequences=True, input_shape=(sequence_length, 1)),
          Dropout(0.3),
          LSTM(150, return_sequences=True),
          Dropout(0.3),
          LSTM(100, return_sequences=True),
          Dropout(0.3),
          LSTM(75, return_sequences=False),
          Dropout(0.3),
          Dense(125, activation='relu'),
          Dense(75, activation='relu'),
          Dense(50, activation='relu'),
          Dense(1)  # Predict single value (Close price)
      ])

  model.compile(optimizer='adam', loss=tf.keras.losses.Huber(), metrics=['mae'])

  #  Learning rate reduction (adjust learning rate when training plateaus)
  lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)

  #  Early stopping (stop if no improvement after 10 epochs)
  early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

  #  Train model with larger batch size for stability
  history = model.fit(X_train, y_train, batch_size=16, epochs=extra_epochs, validation_split=0.1, callbacks=[early_stop, lr_scheduler])

  # Save model
  model.save(model_filename)
  print(f"Model updated and saved as '{model_filename}'")

  #  Predict next 30 days
  last_sequence = scaled_data[-sequence_length:]
  future_predictions = []

  for _ in range(30):
      last_sequence_reshaped = np.reshape(last_sequence, (1, sequence_length, 1))
      predicted_price = model.predict(last_sequence_reshaped)[0, 0]
      future_predictions.append(predicted_price)

      # Update sequence with new prediction
      last_sequence = np.append(last_sequence[1:], predicted_price).reshape(sequence_length, 1)

  # Convert predictions back to original scale
  future_predictions = scaler.inverse_transform(np.array(future_predictions).reshape(-1, 1)).flatten()

  #  Save predictions to JSON
  file_name = ticker  + ".json"
  predictions_json = json.dumps(future_predictions.tolist(), indent=3)
  with open(file_name, "w") as f:
      f.write(predictions_json)

  print("Predictions saved to a json file'")

  #  Plot actual vs predicted prices
  plt.figure(figsize=(12, 6))
  plt.plot(df.index[-len(y_train):], scaler.inverse_transform(y_train.reshape(-1, 1)), label="Actual Prices", color="blue")
  plt.plot(df.index[-len(y_train):], scaler.inverse_transform(model.predict(X_train)), label="Predicted Prices", color="red")
  plt.xlabel("Date")
  plt.ylabel("Close Price (USD)")
  plt.title(ticker + " Stock Price Prediction")
  plt.legend()
  plt.show()

  #  Plot training performance
  plt.figure(figsize=(12, 6))
  plt.plot(history.history['loss'], label='Train Loss', color='blue')
  plt.plot(history.history['val_loss'], label='Validation Loss', color='red')
  plt.xlabel("Epochs")
  plt.ylabel("Loss")
  plt.title("Model Training Performance")
  plt.legend()
  plt.show()